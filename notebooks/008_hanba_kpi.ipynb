{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d490ab9",
   "metadata": {},
   "source": [
    "Ten kod oblicza tzw. **współczynnik hańby** Polskiego Sejmu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f96bde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(asctime)s\\t%(message)s')\n",
    "\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa6cff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Optional, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4f051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_splitter import SentenceSplitter, split_text_into_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85dbcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aipolit.utils.globals import AIPOLIT_10TERM_SEJM_TRANSCRIPTS_DIR\n",
    "from hipisejm.stenparser.transcript import SessionTranscript, SpeechReaction, SpeechInterruption, SessionSpeech\n",
    "from hipisejm.stenparser.transcript_utils import leave_only_specific_type_utt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6dc265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aipolit.transcript.occurrence_counter\n",
    "from aipolit.transcript.utt_sentence_splitter import UttSentenceSplitter\n",
    "\n",
    "import importlib\n",
    "importlib.reload(aipolit.transcript.occurrence_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e944585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as pg\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08570240",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSCRIPT_TYPE_TO_PROCESS = 'sejm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa16b662",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts = []\n",
    "\n",
    "processing_dir = os.path.join(AIPOLIT_10TERM_SEJM_TRANSCRIPTS_DIR, TRANSCRIPT_TYPE_TO_PROCESS)\n",
    "for filename in sorted(os.listdir(processing_dir)):\n",
    "    if re.match(r\"^.*\\.xml$\", filename):\n",
    "        filepath = os.path.join(processing_dir, filename)\n",
    "        transcript = SessionTranscript()\n",
    "        transcript.load_from_xml(filepath)\n",
    "        transcripts.append(transcript)\n",
    "        \n",
    "print(f\"We have loaded {len(transcripts)} transcripts from: {processing_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31eecc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "HANBA_TOKENS = [\n",
    "    'hańba',\n",
    "    'hańby',\n",
    "    'hańbie',\n",
    "    'hańbę',\n",
    "    'hańbą',\n",
    "    'hańbie',\n",
    "    'hańbo',\n",
    "    'hańb',\n",
    "    'hańbom',\n",
    "    'hańbami',\n",
    "    'hańbach',\n",
    "    'haniebny',\n",
    "    'haniebna',     \n",
    "    'haniebne',\n",
    "    'haniebnymi',\n",
    "    'haniebnych',\n",
    "]\n",
    "\n",
    "ZDRADA_TOKENS = [\n",
    "    'zdrada',\n",
    "    'zdrady',\n",
    "    'zdradzie',\n",
    "    'zdradę',\n",
    "    'zdradą',\n",
    "    'zdrado',\n",
    "    'zdrajca',\n",
    "    'zdrajcy',\n",
    "    'zdrajcę',\n",
    "    'zdrajcą',\n",
    "    'zdrajco',\n",
    "    'zdrajców',\n",
    "    'zdrajcom',\n",
    "    'zdrajcami',\n",
    "    'zdrajcach',\n",
    "    'zdradziecki',\n",
    "    'zdradziecka',\n",
    "    'zdradzieckie',\n",
    "    'zdradzieckimi',\n",
    "    'zdradzieckich',\n",
    "]\n",
    "\n",
    "AGENT_TOKENS = [\n",
    "    'agent',\n",
    "    'agenta',\n",
    "    'agentowi',\n",
    "    'agentem',\n",
    "    'agencie',\n",
    "    'agenci',\n",
    "    'agentów',\n",
    "    'agentom',\n",
    "    'agentami',\n",
    "    'agentach',\n",
    "    'agentura',\n",
    "    'agentury',\n",
    "    'agenturze',\n",
    "    'agenturę',\n",
    "    'agenturą',\n",
    "    'agenturo',\n",
    "    'agentur',\n",
    "    'agenturom',\n",
    "    'agentury',\n",
    "    'agenturami',\n",
    "    'agenturach',\n",
    "]\n",
    "\n",
    "WSTYD_TOKENS = [\n",
    "    'wstyd',\n",
    "    'wstydu',\n",
    "    'wstydowi',\n",
    "    'wstydem',\n",
    "    'wstydzie',\n",
    "    'wstydów',\n",
    "    'wstydach',\n",
    "    'wstydami',\n",
    "    'wstydy',\n",
    "]\n",
    "\n",
    "\n",
    "hanba_counter = aipolit.transcript.occurrence_counter.ListOccurrenceCounter(HANBA_TOKENS)\n",
    "zdrada_counter = aipolit.transcript.occurrence_counter.ListOccurrenceCounter(ZDRADA_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3b27aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hanba_results = []\n",
    "zdrada_results = []\n",
    "for transcript in tqdm(transcripts):\n",
    "    hresult = hanba_counter.run_count(transcript)\n",
    "    zresult = zdrada_counter.run_count(transcript)\n",
    "    hanba_results.append(hresult)\n",
    "    zdrada_results.append(zresult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1e7b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts_dates = []\n",
    "\n",
    "total_hanbas_count = []\n",
    "total_zdradas_count = []\n",
    "for transcript, hanba_occur, zdrada_occur in zip(transcripts, hanba_results, zdrada_results):\n",
    "    transcripts_dates.append(transcript.session_date)\n",
    "    total_hanbas_count.append(len(hanba_occur))\n",
    "    total_zdradas_count.append(len(zdrada_occur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d3765b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plot_chart_with_hanba_zdrada(title, dates, data1, name1, data2, name2):\n",
    "    counter_data_df = pd.DataFrame({\n",
    "        'date': dates, \n",
    "        'val1': data1, \n",
    "        'val2': data2},\n",
    "    )\n",
    "\n",
    "    counter_data_df['date'] = counter_data_df['date'].astype(str)\n",
    "    plot = pg.Figure(data=[\n",
    "        pg.Bar(\n",
    "            name = name1,\n",
    "            x = counter_data_df['date'],\n",
    "            y = counter_data_df['val1'],\n",
    "        ),\n",
    "        pg.Bar(\n",
    "            name = name2,\n",
    "            x = counter_data_df['date'],\n",
    "            y = counter_data_df['val2']\n",
    "       )\n",
    "    ])\n",
    "    plot.update_xaxes(type='category')\n",
    "        \n",
    "    plot.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=\"Data posiedzenia\",\n",
    "        yaxis_title=\"Liczba wystąpień\",\n",
    "    )         \n",
    "    plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c040f93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_plot_chart_with_hanba_zdrada(\n",
    "    \"Liczba wystąpień 'Hańba, zdrada' na posiedzieniach Sejmu X kadencji\",\n",
    "    transcripts_dates,\n",
    "    total_hanbas_count,\n",
    "    'Hańba',\n",
    "    total_zdradas_count,\n",
    "    'Zdrada',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef558d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"HAŃBA: Ogółem wystąpień od początku kadencji: {sum(total_hanbas_count)} na {len(transcripts)} dni posiedzeń.\")\n",
    "print(f\"Daje to średnio {sum(total_hanbas_count)/len(total_hanbas_count)} hańby na dzień posiedzenia.\")\n",
    "print()\n",
    "print(f\"ZDRADA: Ogółem wystąpień od początku kadencji: {sum(total_zdradas_count)} na {len(transcripts)} dni posiedzeń.\")\n",
    "print(f\"Daje to średnio {sum(total_zdradas_count)/len(total_zdradas_count)} zdrajców na dzień posiedzenia.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b53c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_reactions_only(results):\n",
    "    reactions = []\n",
    "    for transcript, occurrences in zip(transcripts, results):\n",
    "        reactions_count = 0\n",
    "        for occurrence in occurrences:\n",
    "            if isinstance(occurrence.utt_ref, SpeechReaction) or  isinstance(occurrence.utt_ref, SpeechInterruption):\n",
    "                reactions_count += 1\n",
    "        reactions.append(reactions_count)\n",
    "    return reactions\n",
    "        \n",
    "total_hanbas_reactions_count = count_reactions_only(hanba_results)\n",
    "total_zdradas_reactions_count = count_reactions_only(zdrada_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43741cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_plot_chart_with_hanba_zdrada(\n",
    "    \"Liczba wystąpień 'Hańba, zdrada' jako reakcja na czyjeś wystąpienie na posiedzieniach Sejmu X kadencji\",\n",
    "    transcripts_dates,\n",
    "    total_hanbas_reactions_count,\n",
    "    'Hańba',\n",
    "    total_zdradas_reactions_count,\n",
    "    'Zdrada',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91021d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"HAŃBA: Ogółem wystąpień jako przerwanie innej wypowiedzi od początku kadencji: {sum(total_hanbas_reactions_count)} na {len(transcripts)} dni posiedzeń.\")\n",
    "print(f\"Daje to średnio {sum(total_hanbas_reactions_count)/len(total_hanbas_reactions_count)} przerwanej hańby na dzień posiedzenia.\")\n",
    "print()\n",
    "print(f\"ZDRADA: Ogółem wystąpień jako przerwanie innej wypowiedzi od początku kadencji: {sum(total_zdradas_reactions_count)} na {len(transcripts)} dni posiedzeń.\")\n",
    "print(f\"Daje to średnio {sum(total_zdradas_reactions_count)/len(total_zdradas_reactions_count)} przerwanej zdrady na dzień posiedzenia.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b19b675",
   "metadata": {},
   "source": [
    "Przodownicy hańby i zdrady\n",
    "=======================\n",
    "Sprawdzimy w czyich ustach najczęściej pojawia się słowo hańba i zdrada!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2456f46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_speaker_to_occurrences(dates, results):\n",
    "    speaker_to_occurrences = defaultdict(list)\n",
    "\n",
    "    for date, occur in zip(dates, results):   \n",
    "        for occurrence in occur:\n",
    "            speaker_to_occurrences[occurrence.speaker].append((date, occurrence))\n",
    "    return speaker_to_occurrences\n",
    "\n",
    "hanba_speaker_to_occurrences = count_speaker_to_occurrences(transcripts_dates, hanba_results)\n",
    "zdrada_speaker_to_occurrences = count_speaker_to_occurrences(transcripts_dates, zdrada_results)\n",
    "                                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067c0249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ranking(speaker_to_occurrences):\n",
    "    ranking = 0\n",
    "    raw_data = []\n",
    "    for speaker, occurrences in speaker_to_occurrences.items():\n",
    "        dates_set = set([d for d, _ in occurrences])\n",
    "        raw_data.append((speaker, len(occurrences), len(dates_set)))\n",
    "    \n",
    "    ranking_data = []\n",
    "    for i, entry in enumerate(sorted(raw_data, key=lambda x: (-x[1], -x[2], x[0]))):\n",
    "        ranking_data.append((i+1, *entry))\n",
    "    return ranking_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9554f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hanba_ranking = create_ranking(hanba_speaker_to_occurrences)\n",
    "zdrada_ranking = create_ranking(zdrada_speaker_to_occurrences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8752688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_ranking(title, ranking_data):       \n",
    "    data_df = pd.DataFrame(ranking_data, columns=['No.', 'Mówca', 'Liczba wystąpień', \"W różnych dniach\"])\n",
    "    pd.set_option('display.max_rows', len(data_df))\n",
    "    print()\n",
    "    print(title)\n",
    "    display(data_df.style.hide())\n",
    "    pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91921b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_ranking(\"Ranking osób (nad)używających 'Hańba'\", hanba_ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb913e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_ranking(\"Ranking osób (nad)używających 'Zdrada'\", zdrada_ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea301b5c",
   "metadata": {},
   "source": [
    "Wszystkie wypowiedzenia\n",
    "======================\n",
    "\n",
    "Teraz sprawdźmy sobie jakiego rodzaju wypowiedzi zawierają hańba i zdrada.\n",
    "\n",
    "Uwaga... to jest dużo tekstu :)\n",
    "\n",
    "Dla przejrzystości wypowiedzi powtarzające się zostały scalone w jedno (i poprzedzone liczbą wystąpień). Kiedy wypowiedź jest unikalna wyświetlimy też sobie datę, aby można było sprawdzić szerszy kontekst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74debf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_all_sentences(ranking_data, speaker_to_occurrences):\n",
    "    raw_data = []\n",
    "    for ranking_entry in ranking_data:\n",
    "        ranking = ranking_entry[0]\n",
    "        speaker = ranking_entry[1]\n",
    "        occurrences = speaker_to_occurrences[speaker]\n",
    "        sentence_to_dates = defaultdict(list)\n",
    "        for occur in occurrences:\n",
    "            date = occur[0]\n",
    "            phrase_occur = occur[1]\n",
    "            sentence = phrase_occur.sentence\n",
    "            if isinstance(phrase_occur.utt_ref, SpeechInterruption):\n",
    "                sentence = f\"[przerywa] {sentence}\"\n",
    "                \n",
    "            sentence_to_dates[sentence].append(date)\n",
    "        sentences_sorted = []\n",
    "        for sentence, dates in sorted(sentence_to_dates.items(), key = lambda k_v: (-len(k_v[1]), k_v[1][0])):\n",
    "            if 1 == len(dates):\n",
    "                sentences_sorted.append(f\"({dates[0]}) {sentence}\")\n",
    "            else:\n",
    "                sentences_sorted.append(f\"[x{len(dates)}] ({dates[0]}, ...) {sentence}\")\n",
    "        sentences_sorted_merged = \"<br>\".join(sentences_sorted)\n",
    "        raw_data.append((ranking, speaker, sentences_sorted_merged))\n",
    "    \n",
    "    data_df = pd.DataFrame(raw_data, columns=['No.', 'Mówca', 'Wystąpienia'])\n",
    "    pd.set_option('display.max_rows', len(data_df))\n",
    "    df_styler = data_df.style.set_properties(**{'text-align': 'left'}).set_table_styles([ dict(selector='th', props=[('text-align', 'left')] ) ])\n",
    "    display(df_styler.hide())\n",
    "    pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0164c76",
   "metadata": {},
   "source": [
    "Wszystkie wypowiedzenia z 'Hańbą'\n",
    "================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b462f354",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_all_sentences(hanba_ranking, hanba_speaker_to_occurrences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e209a4",
   "metadata": {},
   "source": [
    "Wszystkie wypowiedzenia ze 'Zdradą'\n",
    "================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343ccf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_all_sentences(zdrada_ranking, zdrada_speaker_to_occurrences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9922e9a",
   "metadata": {},
   "source": [
    "Komu się najczęściej przerywa?\n",
    "==============================\n",
    "\n",
    "W tej części sprawdzimy czyje wystąpienia są najczęściej przerywane okrzykami hańba i zdrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f7ed49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_speaker_to_interruptions(dates, results):\n",
    "    speaker_to_interruptions = defaultdict(list)\n",
    "    for date, occurrences in zip(dates, results):\n",
    "        for occurrence in occurrences:\n",
    "            if isinstance(occurrence.utt_ref, SpeechInterruption):\n",
    "                interrupted_speaker = occurrence.speech_ref.speaker\n",
    "                speaker_to_interruptions[interrupted_speaker].append((date, occurrence))\n",
    "    return speaker_to_interruptions\n",
    "\n",
    "hanba_speaker_to_interruptions = count_speaker_to_interruptions(transcripts_dates, hanba_results)\n",
    "zdrada_speaker_to_interruptions = count_speaker_to_interruptions(transcripts_dates, zdrada_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df075de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_long(sent, length=100):\n",
    "    if len(sent) <= length:\n",
    "        return sent\n",
    "    else:\n",
    "        return sent[0:length] + ' [...]'\n",
    "\n",
    "def show_ranking_of_interruptions(speaker_to_interruptions):\n",
    "    raw_data = []\n",
    "    ranking = 0\n",
    "    for speaker, interruptions in sorted(speaker_to_interruptions.items(), key= lambda k_v: (-len(k_v[1]), k_v[0])):\n",
    "        ranking += 1\n",
    "        \n",
    "        who_interrupted = []\n",
    "        for entry in interruptions:\n",
    "            date = entry[0]\n",
    "            occurrence = entry[1]            \n",
    "            who_interrupted.append(\n",
    "                f\"({date}) {occurrence.speaker}: {occurrence.sentence} [po zdaniu] {cut_long(occurrence.prev_sentence, 150)}\")\n",
    "            \n",
    "        who_interrupted_merged = \"<br>\".join(who_interrupted)\n",
    "        \n",
    "        raw_data.append((ranking, speaker, len(interruptions), who_interrupted_merged))\n",
    "        \n",
    "    data_df = pd.DataFrame(raw_data, columns=['No.', 'Mówca, któremu przerwano', 'Przerwań', 'Kto przerywa'])\n",
    "    pd.set_option('display.max_rows', len(data_df))\n",
    "    df_styler = data_df.style.set_properties(**{'text-align': 'left'}).set_table_styles([ dict(selector='th', props=[('text-align', 'left')] ) ])\n",
    "    display(df_styler.hide())\n",
    "    pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee9d4b7",
   "metadata": {},
   "source": [
    "Ranking komu najczęściej przerywa się słowami 'Hańba!'\n",
    "=============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741c992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_ranking_of_interruptions(hanba_speaker_to_interruptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6b3e85",
   "metadata": {},
   "source": [
    "Ranking komu najczęściej przerywa się słowami 'Zdrada'\n",
    "=============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efe831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_ranking_of_interruptions(zdrada_speaker_to_interruptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a8e296",
   "metadata": {},
   "source": [
    "Czy przerywane wypowiedzi coś łączy?\n",
    "===================================\n",
    "\n",
    "Materiału powyżej nie ma zbyt dużo, jednakże spróbujemy i tak sprawdzić czy da się znaleźć jakiekolwiek punkty wspólne.\n",
    "\n",
    "W tym celu proponuję następujące podejście:\n",
    "\n",
    "1. Zbierzemy N zdań (nie tylko jedno, aby złapać szerszy kontekst) wypowiedzenia, które zostało przerwane (jeżeli wypowiedzenie jest krótsze, to oczywiście weźmiemy całość wypowiedzi do momentu przerwania).\n",
    "2. Z każdego takiego fragmentu spróbujemy wyciągnąć słowa kluczowe, które następnie posłużą nam do scharakteryzowania każdego wypowiedzenia.\n",
    "3. Sprawdzimy frekwencję pozyskanych słów kluczowych - być może niektóre występują statystycznie częściej\n",
    "4. Dodatkowo dokonamy analizy czy pewne słowa kluczowe nie są do siebie podobne - w tym miejscu wykorzystamy analizę skupień (clustering) i odpowiednio zaktualizujemy ranking. \n",
    "\n",
    "W drugim mniej ambitnym podejściu zapytamy o zdanie LLMa ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4138ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "utt_sentence_splitter = UttSentenceSplitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ac9bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_INTERRUPTED_FRAGMENT_LENGTH = 7 # number of sentences!\n",
    "\n",
    "def process_utt_for_split(result: List[str], utt: str, max_length: int) -> bool:\n",
    "    splitted_list = utt_sentence_splitter.split_utt_to_sentences(utt)\n",
    "    for sentence in reversed(splitted_list):\n",
    "        result.append(sentence)\n",
    "        if len(result) >= max_length:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def retrieve_interrupted_fragment(occurrence, max_length) -> List[str]:\n",
    "    \"\"\"\n",
    "    Retrieces only \"norm\" utts (without interruptions/reactions) from the given speech\n",
    "    to the place where occurrence sentence appears.\n",
    "    Occurrence is not taken into account to the result if it is reaction or interruption!\n",
    "    max_length - max number of sentences to retrieve!\n",
    "    \"\"\"\n",
    "    occurrence_sentence = occurrence.sentence\n",
    "    speech_ref = occurrence.speech_ref\n",
    "    matched_utt_index = occurrence.matched_utt_index\n",
    "    \n",
    "    utts_until_matched = speech_ref.content[:matched_utt_index]\n",
    "    utts_strings_only = leave_only_specific_type_utt(utts_until_matched, str)\n",
    "    \n",
    "    result = []\n",
    "    # now we will split utts into sentences, to leave only max_length\n",
    "    for utt in reversed(utts_strings_only):\n",
    "        if process_utt_for_split(result, utt, max_length):\n",
    "            break\n",
    "    return list(reversed(result))\n",
    "\n",
    "\n",
    "def create_interrupted_fragments(results):\n",
    "    interrupted_fragments = []\n",
    "    for occurrences in results:\n",
    "        for occurrence in occurrences:\n",
    "            if isinstance(occurrence.utt_ref, SpeechInterruption):\n",
    "                interrupted_speaker = occurrence.speech_ref.speaker\n",
    "                \n",
    "                interrupted_fragment = retrieve_interrupted_fragment(occurrence, max_length=MAX_INTERRUPTED_FRAGMENT_LENGTH)\n",
    "                entry = (interrupted_fragment, occurrence)\n",
    "                interrupted_fragments.append(entry)\n",
    "                                \n",
    "    return interrupted_fragments\n",
    "    \n",
    "hanba_interrupted_fragments = create_interrupted_fragments(hanba_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b8e906",
   "metadata": {},
   "source": [
    "Sprawdźmy jeszcze tylko jak dużo tekstu udało nam się wyciągnąć do analizy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae87888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_estimate_of_fragments_size(fragments):\n",
    "    stats = {\n",
    "        'sentences': 0,\n",
    "        'tokens': 0,\n",
    "        'characters': 0,\n",
    "    }\n",
    "    \n",
    "    for frag in fragments:\n",
    "        stats['sentences'] += len(frag[0])\n",
    "        for sentence in frag[0]:\n",
    "            tokens = re.split(r\"\\s+\", sentence)\n",
    "            stats['tokens'] += len(tokens)\n",
    "            stats['characters'] += len(sentence)\n",
    "        \n",
    "    print(f\"Całkowita liczba zdań: {stats['sentences']}\" )\n",
    "    print(f\"Całkowita liczba wyrazów (tokenów): {stats['tokens']}\" )\n",
    "    print(f\"Całkowita liczba znaków: {stats['characters']}\" )\n",
    "    \n",
    "quick_estimate_of_fragments_size(hanba_interrupted_fragments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71989d39",
   "metadata": {},
   "source": [
    "Aby pozyskać słowa kluczowe wykorzystamy moduł: https://github.com/MaartenGr/KeyBERT\n",
    "\n",
    "Każdy z fragmentów zostanie potraktowany jako dokument, z którego pozyskamy słowa kluczowe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3394500",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_doc = \" \".join(hanba_interrupted_fragments[6][0])\n",
    "print(\"Sample doc:\\n\\n\" + sample_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb0389d-6de3-418c-af1b-75e6d0f29b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://github.com/bieli/stopwords/blob/master/polish.stopwords.txt\n",
    "pl_stopwords = \"\"\"a\n",
    "aby\n",
    "ach\n",
    "acz\n",
    "aczkolwiek\n",
    "aj\n",
    "albo\n",
    "ale\n",
    "alez\n",
    "ależ\n",
    "ani\n",
    "az\n",
    "aż\n",
    "bardziej\n",
    "bardzo\n",
    "beda\n",
    "bedzie\n",
    "bez\n",
    "deda\n",
    "będą\n",
    "bede\n",
    "będę\n",
    "będzie\n",
    "bo\n",
    "bowiem\n",
    "by\n",
    "byc\n",
    "być\n",
    "byl\n",
    "byla\n",
    "byli\n",
    "bylo\n",
    "byly\n",
    "był\n",
    "była\n",
    "było\n",
    "były\n",
    "bynajmniej\n",
    "cala\n",
    "cali\n",
    "caly\n",
    "cała\n",
    "cały\n",
    "ci\n",
    "cie\n",
    "ciebie\n",
    "cię\n",
    "co\n",
    "cokolwiek\n",
    "cos\n",
    "coś\n",
    "czasami\n",
    "czasem\n",
    "czemu\n",
    "czy\n",
    "czyli\n",
    "daleko\n",
    "dla\n",
    "dlaczego\n",
    "dlatego\n",
    "do\n",
    "dobrze\n",
    "dokad\n",
    "dokąd\n",
    "dosc\n",
    "dość\n",
    "duzo\n",
    "dużo\n",
    "dwa\n",
    "dwaj\n",
    "dwie\n",
    "dwoje\n",
    "dzis\n",
    "dzisiaj\n",
    "dziś\n",
    "gdy\n",
    "gdyby\n",
    "gdyz\n",
    "gdyż\n",
    "gdzie\n",
    "gdziekolwiek\n",
    "gdzies\n",
    "gdzieś\n",
    "go\n",
    "i\n",
    "ich\n",
    "ile\n",
    "im\n",
    "inna\n",
    "inne\n",
    "inny\n",
    "innych\n",
    "iz\n",
    "iż\n",
    "ja\n",
    "jak\n",
    "jakas\n",
    "jakaś\n",
    "jakby\n",
    "jaki\n",
    "jakichs\n",
    "jakichś\n",
    "jakie\n",
    "jakis\n",
    "jakiś\n",
    "jakiz\n",
    "jakiż\n",
    "jakkolwiek\n",
    "jako\n",
    "jakos\n",
    "jakoś\n",
    "ją\n",
    "je\n",
    "jeden\n",
    "jedna\n",
    "jednak\n",
    "jednakze\n",
    "jednakże\n",
    "jedno\n",
    "jego\n",
    "jej\n",
    "jemu\n",
    "jesli\n",
    "jest\n",
    "jestem\n",
    "jeszcze\n",
    "jeśli\n",
    "jezeli\n",
    "jeżeli\n",
    "juz\n",
    "już\n",
    "kazdy\n",
    "każdy\n",
    "kiedy\n",
    "kilka\n",
    "kims\n",
    "kimś\n",
    "kto\n",
    "ktokolwiek\n",
    "ktora\n",
    "ktore\n",
    "ktorego\n",
    "ktorej\n",
    "ktory\n",
    "ktorych\n",
    "ktorym\n",
    "ktorzy\n",
    "ktos\n",
    "ktoś\n",
    "która\n",
    "które\n",
    "którego\n",
    "której\n",
    "który\n",
    "których\n",
    "którym\n",
    "którzy\n",
    "ku\n",
    "lat\n",
    "lecz\n",
    "lub\n",
    "ma\n",
    "mają\n",
    "mało\n",
    "mam\n",
    "mi\n",
    "miedzy\n",
    "między\n",
    "mimo\n",
    "mna\n",
    "mną\n",
    "mnie\n",
    "moga\n",
    "mogą\n",
    "moi\n",
    "moim\n",
    "moj\n",
    "moja\n",
    "moje\n",
    "moze\n",
    "mozliwe\n",
    "mozna\n",
    "może\n",
    "możliwe\n",
    "można\n",
    "mój\n",
    "mu\n",
    "musi\n",
    "my\n",
    "na\n",
    "nad\n",
    "nam\n",
    "nami\n",
    "nas\n",
    "nasi\n",
    "nasz\n",
    "nasza\n",
    "nasze\n",
    "naszego\n",
    "naszych\n",
    "natomiast\n",
    "natychmiast\n",
    "nawet\n",
    "nia\n",
    "nią\n",
    "nic\n",
    "nich\n",
    "nie\n",
    "niech\n",
    "niego\n",
    "niej\n",
    "niemu\n",
    "nigdy\n",
    "nim\n",
    "nimi\n",
    "niz\n",
    "niż\n",
    "no\n",
    "o\n",
    "obok\n",
    "od\n",
    "około\n",
    "on\n",
    "ona\n",
    "one\n",
    "oni\n",
    "ono\n",
    "oraz\n",
    "oto\n",
    "owszem\n",
    "pan\n",
    "pana\n",
    "pani\n",
    "po\n",
    "pod\n",
    "podczas\n",
    "pomimo\n",
    "ponad\n",
    "poniewaz\n",
    "ponieważ\n",
    "powinien\n",
    "powinna\n",
    "powinni\n",
    "powinno\n",
    "poza\n",
    "prawie\n",
    "przeciez\n",
    "przecież\n",
    "przed\n",
    "przede\n",
    "przedtem\n",
    "przez\n",
    "przy\n",
    "roku\n",
    "rowniez\n",
    "również\n",
    "sam\n",
    "sama\n",
    "są\n",
    "sie\n",
    "się\n",
    "skad\n",
    "skąd\n",
    "soba\n",
    "sobą\n",
    "sobie\n",
    "sposob\n",
    "sposób\n",
    "swoje\n",
    "ta\n",
    "tak\n",
    "taka\n",
    "taki\n",
    "takie\n",
    "takze\n",
    "także\n",
    "tam\n",
    "te\n",
    "tego\n",
    "tej\n",
    "ten\n",
    "teraz\n",
    "też\n",
    "to\n",
    "toba\n",
    "tobą\n",
    "tobie\n",
    "totez\n",
    "toteż\n",
    "totobą\n",
    "trzeba\n",
    "tu\n",
    "tutaj\n",
    "twoi\n",
    "twoim\n",
    "twoj\n",
    "twoja\n",
    "twoje\n",
    "twój\n",
    "twym\n",
    "ty\n",
    "tych\n",
    "tylko\n",
    "tym\n",
    "u\n",
    "w\n",
    "wam\n",
    "wami\n",
    "was\n",
    "wasz\n",
    "wasza\n",
    "wasze\n",
    "we\n",
    "według\n",
    "wiele\n",
    "wielu\n",
    "więc\n",
    "więcej\n",
    "wlasnie\n",
    "właśnie\n",
    "wszyscy\n",
    "wszystkich\n",
    "wszystkie\n",
    "wszystkim\n",
    "wszystko\n",
    "wtedy\n",
    "wy\n",
    "z\n",
    "za\n",
    "zaden\n",
    "zadna\n",
    "zadne\n",
    "zadnych\n",
    "zapewne\n",
    "zawsze\n",
    "ze\n",
    "zeby\n",
    "zeznowu\n",
    "zł\n",
    "znow\n",
    "znowu\n",
    "znów\n",
    "zostal\n",
    "został\n",
    "żaden\n",
    "żadna\n",
    "żadne\n",
    "żadnych\n",
    "że\n",
    "żeby\"\"\".split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81431c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO disabled\n",
    "\n",
    "from keybert import KeyBERT\n",
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "from collections import Counter\n",
    "\n",
    "# wybierzemy model sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 \n",
    "# rekomendowany przez KeyBert\n",
    "# można też sprawdzić modele z benchmarku https://klejbenchmark.com/leaderboard/\n",
    "kw_model_name = 'allegro/herbert-large-cased'\n",
    "kw_model_name = 'sdadas/polish-roberta-large-v2'\n",
    "kw_model_name = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "\n",
    "kw_model_embedding = TransformerDocumentEmbeddings(kw_model_name)\n",
    "kw_model = KeyBERT(model=kw_model_embedding)\n",
    "\n",
    "all_keywords = Counter()\n",
    "for i, hanba_frag in tqdm(enumerate(hanba_interrupted_fragments)):\n",
    "    # TODO this does not work as good as ChatGPT :(\n",
    "    break\n",
    "    hanba_doc = \" \".join(hanba_frag[0])          \n",
    "    \n",
    "    keywords = kw_model.extract_keywords(\n",
    "        hanba_doc, \n",
    "        keyphrase_ngram_range=(1, 3), \n",
    "        stop_words=pl_stopwords, \n",
    "        highlight=False, \n",
    "        use_maxsum=True, \n",
    "        nr_candidates=10, \n",
    "        top_n=5, \n",
    "        use_mmr=True, \n",
    "        diversity=0.9)\n",
    "    for kw in keywords:\n",
    "        all_keywords[kw[0]] += 1\n",
    "\n",
    "rank = 0\n",
    "for kw, freq in sorted(all_keywords.items(), key=lambda k_v: -k_v[1]):\n",
    "    rank += 1\n",
    "    \n",
    "    print(f\"{rank}. {kw} {freq}\")\n",
    "    if rank > 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804d2c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_for_keybert = []\n",
    "for frag in hanba_interrupted_fragments:\n",
    "    sents = \" \".join(frag[0])\n",
    "    documents_for_keybert.append(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4514d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aipolit.models.keybert_llm_cached import KeyBertLLMCached\n",
    "kw_model = KeyBertLLMCached()\n",
    "\n",
    "keywords = kw_model.extract_keywords(documents_for_keybert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e13622",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keywords = Counter()\n",
    "for doc, keys in zip(documents_for_keybert, keywords):\n",
    "    \n",
    "    print(\"DOCUMENT:\", doc)\n",
    "    print(\"Tematy:\", keys)\n",
    "    for kw in keys:\n",
    "        all_keywords[kw] += 1\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b850308",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 0\n",
    "for kw, freq in sorted(all_keywords.items(), key=lambda k_v: -k_v[1]):\n",
    "    rank += 1\n",
    "    \n",
    "    print(f\"{rank}. {kw} {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6fe2dd-9755-4622-9c8c-3a35a979d5f9",
   "metadata": {},
   "source": [
    "No a teraz chcielibyśmy pogrupować te wszystkie tematy, bo niektóre z nich są bardzo podobne (np. Donald Tusk, premier Donald Tusk, premier Tusk). Do tego wykorzystamy analizę skupień. Ponieważ nie wiemy jaka liczba grup jest optymalna użyjemy algorytmu DBScan, który pozwala łączyć elementy w grupy tak długo, aż nie zostanie przekroczona pewna wartość graniczna. Hiperparametry alogrytmu dobrałem metodą prób i błędów :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423924b9-9907-475a-974c-8cbcb118da84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.manifold import TSNE\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def cluster_topics(topics, show_plot=True):    \n",
    "    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "    embeddings = model.encode(topics)\n",
    "\n",
    "    # hyperparamters are chosen after some experiments\n",
    "    dbscan = DBSCAN(eps=0.15, min_samples=2, metric='cosine')\n",
    "    clusters = dbscan.fit_predict(embeddings)\n",
    "\n",
    "    if show_plot:\n",
    "        tsne = TSNE(n_components=2, random_state=42)\n",
    "        embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        unique_labels = set(clusters)\n",
    "        colors = plt.cm.rainbow(np.linspace(0, 1, len(unique_labels)))\n",
    "\n",
    "        for label, color in zip(unique_labels, colors):\n",
    "            if label == -1:\n",
    "                color = 'gray'\n",
    "                marker = 'x'\n",
    "            else:\n",
    "                marker = 'o'\n",
    "    \n",
    "            plt.scatter(\n",
    "                embeddings_2d[clusters == label, 0],\n",
    "                embeddings_2d[clusters == label, 1],\n",
    "                c=[color],\n",
    "                marker=marker,\n",
    "                label=f'Klaster {label}' if label != -1 else 'Szum'\n",
    "            )\n",
    "\n",
    "        plt.title('Analiza skupień tematów (DBSCAN)')\n",
    "        plt.show()\n",
    "\n",
    "    cluster_to_topics = defaultdict(list)\n",
    "    for cluster_id, topic in zip(clusters, topics):\n",
    "        cluster_to_topics[cluster_id].append(topic)\n",
    "    return cluster_to_topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7e05a9-6d3e-46cb-bab1-cb159f44e437",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_cluster = [k for k in all_keywords.keys() if k != 'brak tematów']\n",
    "\n",
    "cluster_to_topics = cluster_topics(to_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c3b1ef-02e3-4109-bd92-26603d94c3c7",
   "metadata": {},
   "source": [
    "Zobaczymy jakiego rodzaju grupy powstały w wyniku działania alogrytmu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534fcd7a-3d20-4c15-9c05-27bd148d3ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_clustered_topics(cluster_to_topics):\n",
    "    rank = 0\n",
    "    for cls_id, topics in sorted(cluster_to_topics.items(), key=lambda k_v: -len(k_v[1])):\n",
    "        if cls_id < 0:\n",
    "            continue\n",
    "        rank += 1\n",
    "        print(f\"{rank}. Liczba tematów: {len(topics)}. Tematy: {', '.join(topics)}\")\n",
    "\n",
    "\n",
    "print_clustered_topics(cluster_to_topics)         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfaab90-e9dd-4c19-bc17-974721949703",
   "metadata": {},
   "source": [
    "Pozostaje teraz policzyć frekwencję wystąpień tematów w ramach każdej z grup. To da nam ranking tematów, które pojawiają się najczęściej przed reakcją \"Hańba!\". No i trzeba teraz rozbić sztuczny klaster -1 na osobne grupy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c892022d-3da0-4143-a7f1-4ba5ca7e8ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_to_freq_and_topics = dict()\n",
    "\n",
    "# na początek rozbijemy klaster -1 na pojedyncze grupy\n",
    "for topic in cluster_to_topics[-1]:\n",
    "    new_id = -(len(cluster_to_freq_and_topics) + 1)\n",
    "    cluster_to_freq_and_topics[new_id] = ([topic], all_keywords[topic])\n",
    "\n",
    "# a teraz dodajmy większe klastry\n",
    "for cluster_id, topics in cluster_to_topics.items():\n",
    "    if cluster_id < 0:\n",
    "        continue\n",
    "    freq = 0\n",
    "    for topic in topics:\n",
    "        freq += all_keywords[topic]\n",
    "    cluster_to_freq_and_topics[cluster_id] = (topics, freq)\n",
    "\n",
    "rank = 0\n",
    "for cluster_id, entry in sorted(cluster_to_freq_and_topics.items(), key=lambda k_v: -k_v[1][1]):\n",
    "    rank += 1\n",
    "    print(f\"{rank}. Wystąpień: {entry[1]}. Liczba tematów: {len(entry[0])}. Tematy: {', '.join(entry[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8efd6c8-b924-4fb6-a84f-42e68e9c89b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-polit",
   "language": "python",
   "name": "ai-polit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
